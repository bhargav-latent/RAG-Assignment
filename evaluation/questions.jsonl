{"id": "latent_q001", "question": "What is the formula for the reverse-time stochastic differential equation used in diffusion models?", "ground_truth": "dx_t = [f_t * x_t - ((1 + eta^2) / 2) * g_t^2 * ∇_{x_t} log p(x_t)] dt + eta * g_t * dw_t, where eta >= 0 is a parameter controlling stochasticity, and the coefficients f_t and g_t are derived from alpha_t and sigma_t.", "category": "precision", "source_documents": ["Latent Diffusion Models for Physics Emulation"], "metadata": {"difficulty": "hard", "reasoning_type": "single-hop"}}
{"id": "latent_q002", "question": "What are the three datasets from TheWell used in the latent diffusion models study?", "ground_truth": "The three datasets are: (1) Euler Multi-Quadrants - solving compressible Euler equations with 512x512 grid and 5 channels, (2) Rayleigh-Bénard (RB) - convection with 512x128 grid and 4 channels, and (3) Turbulence Gravity Cooling (TGC) - interstellar medium with 64x64x64 grid and 6 channels.", "category": "recall", "source_documents": ["Latent Diffusion Models for Physics Emulation"], "metadata": {"difficulty": "medium", "reasoning_type": "single-hop"}}
{"id": "latent_q003", "question": "What is the formula for Tweedie's formula linking the optimal denoiser to the score function?", "ground_truth": "E[x|x_t] = (x_t + sigma_t^2 * ∇_{x_t} log p(x_t)) / alpha_t, which allows using s_phi(x_t) = sigma_t^{-2} * (d_phi(x_t, t) - alpha_t * x_t) as a score estimate.", "category": "precision", "source_documents": ["Latent Diffusion Models for Physics Emulation"], "metadata": {"difficulty": "hard", "reasoning_type": "single-hop"}}
{"id": "latent_q004", "question": "What compression rates were tested for the Euler dataset in the latent diffusion models paper?", "ground_truth": "The compression rates tested for the Euler dataset were 80 (64 latent channels), 320 (16 latent channels), and 1280 (4 latent channels). The autoencoder used a spatial downsampling factor r=32 for 2D datasets.", "category": "precision", "source_documents": ["Latent Diffusion Models for Physics Emulation"], "metadata": {"difficulty": "medium", "reasoning_type": "single-hop"}}
{"id": "latent_q005", "question": "What is the formula for variance-normalized RMSE (VRMSE) used to evaluate emulation accuracy?", "ground_truth": "VRMSE(u, v) = sqrt(<(u - v)^2> / (<(u - <u>)^2> + epsilon)), where <·> denotes the spatial mean operator and epsilon = 10^{-6} is a numerical stability term.", "category": "precision", "source_documents": ["Latent Diffusion Models for Physics Emulation"], "metadata": {"difficulty": "medium", "reasoning_type": "single-hop"}}
{"id": "well_q001", "question": "What is the formula for the acoustic scattering pressure evolution equation in The Well dataset?", "ground_truth": "∂p/∂t + K(x,y) * (∂u/∂x + ∂v/∂y) = 0, where p is pressure, K is bulk modulus (constant at 4), u and v are velocities in x and y directions, and rho is the material density.", "category": "precision", "source_documents": ["Science-Datasets"], "metadata": {"difficulty": "medium", "reasoning_type": "single-hop"}}
{"id": "well_q002", "question": "List all the simulation software packages used to generate datasets in The Well collection.", "ground_truth": "The software packages are: Clawpack, Dedalus, Athena++, νbhlight, FDPS (Framework for Developing Particle Simulator), ASURA-FDPS, TurMix3D, and custom Python/Fortran/Matlab implementations.", "category": "recall", "source_documents": ["Science-Datasets"], "metadata": {"difficulty": "medium", "reasoning_type": "single-hop"}}
{"id": "well_q003", "question": "What are the exact formulas for the Gray-Scott reaction-diffusion equations?", "ground_truth": "∂A/∂t = delta_A * ΔA - AB^2 + f(1 - A) and ∂B/∂t = delta_B * ΔB + AB^2 - (f + k)B, where A and B are chemical species concentrations, delta_A and delta_B are diffusion constants, f is the feed rate, and k is the kill rate.", "category": "precision", "source_documents": ["Science-Datasets"], "metadata": {"difficulty": "hard", "reasoning_type": "single-hop"}}
{"id": "well_q004", "question": "What are all the field types present in the Euler Multi-Quadrants dataset?", "ground_truth": "The fields are: density (scalar field), energy (scalar field), pressure (scalar field), and momentum (vector field). The 2D state has 5 total channels: three scalar fields (energy, density, pressure) and one vector field (momentum with 2 components).", "category": "recall", "source_documents": ["Science-Datasets"], "metadata": {"difficulty": "easy", "reasoning_type": "single-hop"}}
{"id": "well_q005", "question": "What is the total size of The Well dataset collection and how many datasets does it contain?", "ground_truth": "The Well is a collection of 16 simulation datasets totaling 15TB (terabytes). The datasets are in two and three spatial dimensions and follow a common schema accessible through a unified interface stored in HDF5 format.", "category": "precision", "source_documents": ["Science-Datasets"], "metadata": {"difficulty": "easy", "reasoning_type": "single-hop"}}
{"id": "usecase_q001", "question": "What are the three main parts of a use case according to Cockburn's writing effective use cases?", "ground_truth": "The three main parts are: (1) Preconditions - what must be true before the use case starts, (2) Main Success Scenario (MSS) - the primary flow of events leading to the goal, and (3) Postconditions - what must be true after the use case completes successfully.", "category": "recall", "source_documents": ["Writing_effective_use_cases"], "metadata": {"difficulty": "medium", "reasoning_type": "single-hop"}}
{"id": "usecase_q002", "question": "List all the levels of use cases in Cockburn's framework from highest to lowest.", "ground_truth": "From highest to lowest: Cloud level (summary level covering many user goals), Kite level (summary of multiple user goals), Sea level (user goal level - main focus), Fish level (subfunction supporting a user goal), and Clam level (sub-subfunction or internal implementation detail).", "category": "recall", "source_documents": ["Writing_effective_use_cases"], "metadata": {"difficulty": "hard", "reasoning_type": "single-hop"}}
{"id": "usecase_q003", "question": "What is the definition and purpose of the Primary Actor in a use case?", "ground_truth": "The Primary Actor is the stakeholder who calls on the system to deliver its service. The primary actor has a goal with respect to the system and is the one who initiates the use case to achieve that goal. The system delivers value to the primary actor.", "category": "precision", "source_documents": ["Writing_effective_use_cases"], "metadata": {"difficulty": "medium", "reasoning_type": "single-hop"}}
{"id": "usecase_q004", "question": "What are all the types of stakeholders and interests that should be identified for a use case?", "ground_truth": "Stakeholders include: Primary Actor (initiates and has main goal), Secondary Actors (provide services to the system), Supporting Actors (help the primary actor), Offstage Stakeholders (have interests but don't interact), and the System Under Discussion (SuD) itself. Each stakeholder's interests and success criteria should be documented.", "category": "recall", "source_documents": ["Writing_effective_use_cases"], "metadata": {"difficulty": "hard", "reasoning_type": "multi-hop"}}
{"id": "usecase_q005", "question": "What is the difference between extensions and alternative flows in use cases?", "ground_truth": "Extensions (also called exception flows) handle conditions where the main success scenario fails or encounters an error that prevents achieving the goal. Alternative flows represent valid variations of the main scenario that still successfully achieve the use case goal through a different path.", "category": "precision", "source_documents": ["Writing_effective_use_cases"], "metadata": {"difficulty": "medium", "reasoning_type": "single-hop"}}
{"id": "vsfsm_q001", "question": "What is the formula for binding operation in Vector Symbolic Architectures?", "ground_truth": "The binding operation combines two vectors x and y using element-wise multiplication: z = x ⊙ y, where ⊙ represents element-wise (Hadamard) product. This creates a dissimilar vector z that can encode the relationship between x and y, and is approximately orthogonal to both inputs.", "category": "precision", "source_documents": ["vector symbolic finate state machines in Attractor Neural Networks"], "metadata": {"difficulty": "medium", "reasoning_type": "single-hop"}}
{"id": "vsfsm_q002", "question": "List all the fundamental operations in Vector Symbolic Architectures (VSAs).", "ground_truth": "The fundamental VSA operations are: (1) Binding - combining vectors to represent relations (element-wise multiplication), (2) Bundling - superposition of multiple vectors (element-wise addition), (3) Permutation - reordering vector elements for sequences, and (4) Similarity measurement - typically using cosine similarity or dot product.", "category": "recall", "source_documents": ["vector symbolic finate state machines in Attractor Neural Networks"], "metadata": {"difficulty": "medium", "reasoning_type": "single-hop"}}
{"id": "vsfsm_q003", "question": "What is the mathematical definition of an attractor in the context of attractor neural networks?", "ground_truth": "An attractor is a stable state x* in the network's state space where the dynamics converge. Mathematically, it satisfies f(x*) = x* (a fixed point), and perturbations in the basin of attraction return to x*. The basin of attraction is the region of states that converge to that attractor.", "category": "precision", "source_documents": ["vector symbolic finate state machines in Attractor Neural Networks"], "metadata": {"difficulty": "hard", "reasoning_type": "single-hop"}}
{"id": "vsfsm_q004", "question": "What are all the components needed to implement a finite state machine using attractor neural networks and VSAs?", "ground_truth": "The components are: (1) State vectors - high-dimensional vectors representing each symbolic state, (2) Transition vectors - encoding state transitions using binding, (3) Attractor dynamics - providing stable states and transitions between them, (4) Input vectors - representing input symbols, (5) Basin of attraction - ensuring robustness to noise, and (6) Update rule - governing the network dynamics to move between attractors.", "category": "recall", "source_documents": ["vector symbolic finate state machines in Attractor Neural Networks"], "metadata": {"difficulty": "hard", "reasoning_type": "multi-hop"}}
{"id": "vsfsm_q005", "question": "How do Hopfield networks relate to implementing vector symbolic finite state machines?", "ground_truth": "Hopfield networks serve as attractor neural networks where stored patterns become attractors. In VSA-based FSMs, each symbolic state is represented as an attractor (stored pattern) in the Hopfield network. The network's energy function ensures convergence to these attractors, and the basin of attraction provides noise tolerance. State transitions are implemented through the network's dynamics guided by input and transition vectors.", "category": "precision", "source_documents": ["vector symbolic finate state machines in Attractor Neural Networks"], "metadata": {"difficulty": "hard", "reasoning_type": "multi-hop"}}
{"id": "cross_q001", "question": "How does the Euler Multi-Quadrants dataset from The Well compare to its usage in the latent diffusion models paper in terms of resolution, fields, and simulation parameters?", "ground_truth": "Both papers use the same Euler Multi-Quadrants dataset with identical specifications: 512x512 grid resolution, 5 channels (density, energy, pressure, and 2D momentum vector), modeling compressible Euler equations. The Well paper specifies the simulation uses Clawpack software and varies gamma (heat capacity ratio) with values including {1.3, 1.4, 1.13, 1.22, 1.33, 1.76, 1.365, 1.404, 1.453, 1.597} and boundary conditions (open or periodic). The latent diffusion paper uses this dataset to test compression rates of 80x, 320x, and 1280x with temporal bundling and reports it has 100 time steps with stride delta=4.", "category": "cross-document", "source_documents": ["Science-Datasets", "Latent Diffusion Models for Physics Emulation"], "metadata": {"difficulty": "hard", "reasoning_type": "multi-hop"}}
{"id": "cross_q002", "question": "Comparing evaluation methodologies: What metrics and evaluation approaches are used in The Well datasets versus the latent diffusion models paper?", "ground_truth": "The Well paper establishes VRMSE (variance-normalized RMSE) as a standard metric across all datasets, with formula VRMSE(u,v) = sqrt(<(u-v)^2> / (<(u-<u>)^2> + epsilon)) where epsilon=10^{-6}. The latent diffusion paper adopts this same VRMSE metric and extends it with: (1) Power spectrum RMSE for statistical plausibility across low/mid/high frequency bands, (2) Spread-skill ratio for ensemble quality with formula Skill ≈ sqrt((K+1)/K) * Spread, where K is ensemble size, and (3) evaluation at different lead time horizons (1:20, 21:60, 61:100 for Euler). Both emphasize that VRMSE is preferred over NRMSE because it doesn't down-weight errors in non-negative fields.", "category": "cross-document", "source_documents": ["Science-Datasets", "Latent Diffusion Models for Physics Emulation"], "metadata": {"difficulty": "hard", "reasoning_type": "multi-hop"}}
{"id": "cross_q003", "question": "How could the concepts from writing effective use cases be applied to design the evaluation framework described in The Well datasets paper?", "ground_truth": "Use case principles apply to The Well evaluation design as follows: (1) Primary Actor would be ML researchers using the dataset with goal of benchmarking models, (2) Preconditions include having 15TB storage, compute resources, and understanding of the domain physics, (3) Main Success Scenario involves downloading data, training models, computing VRMSE metrics, and publishing results, (4) Alternative flows cover different tasks like inverse scattering or superresolution beyond autoregressive forecasting, (5) Extensions handle failures like insufficient compute or invalid data, (6) Postconditions ensure reproducible benchmarks are established. The stakeholders include dataset contributors (domain experts), ML researchers (primary actors), and the broader scientific community (offstage stakeholders interested in progress).", "category": "cross-document", "source_documents": ["Writing_effective_use_cases", "Science-Datasets"], "metadata": {"difficulty": "hard", "reasoning_type": "multi-hop"}}
{"id": "cross_q004", "question": "Could vector symbolic architectures and attractor networks be used to represent the state transitions in the dynamical systems from The Well datasets? Explain with specific examples.", "ground_truth": "Yes, VSAs and attractor networks could represent state transitions in The Well systems. For the Euler Multi-Quadrants dataset with discrete shock interactions: (1) Each physical regime (shock, rarefaction, contact discontinuity) could be encoded as a high-dimensional state vector using VSA binding operations, (2) The temporal evolution from state x^i to x^{i+1} could be represented as attractor transitions where each physical configuration is an attractor state, (3) The basin of attraction would provide robustness to numerical noise in the simulation, (4) Transition vectors could encode the physics equations (like compressible Euler equations) using binding of density, pressure, and velocity vectors, (5) The bundling operation could superimpose multiple interacting wave patterns. For Gray-Scott reaction-diffusion, different pattern types (gliders, bubbles, maze, worms, spirals, spots) could each be attractors, with transitions encoding the reaction dynamics AB^2 and diffusion terms.", "category": "cross-document", "source_documents": ["vector symbolic finate state machines in Attractor Neural Networks", "Science-Datasets"], "metadata": {"difficulty": "hard", "reasoning_type": "multi-hop"}}
{"id": "cross_q005", "question": "How do the compression and reconstruction trade-offs in latent diffusion models relate to information preservation principles that might be important when documenting use cases for a physics simulation system?", "ground_truth": "The latent diffusion paper shows that compression rates up to 1280x maintain emulation accuracy despite degraded reconstruction (autoencoder VRMSE increases from 0.011 at 80x to 0.145 at 1280x for Euler), filtering semantically meaningful from perceptually irrelevant patterns. Similarly, use case documentation must balance compression (brevity) with information preservation: (1) Preconditions must be specific and testable (like VRMSE < threshold) not vague, (2) The Main Success Scenario compresses the interaction to essential steps, filtering out implementation details (analogous to latent space filtering high-frequency noise), (3) Extensions document edge cases without overwhelming the reader (like how diffusion models handle uncertainty with spread-skill ratio ≈ 1), (4) Stakeholder interests are 'compressed' to essential concerns. Both domains show that aggressive compression is acceptable if it preserves what matters for the goal - physical accuracy for emulation, goal achievement for use cases - even if surface details differ.", "category": "cross-document", "source_documents": ["Latent Diffusion Models for Physics Emulation", "Writing_effective_use_cases"], "metadata": {"difficulty": "hard", "reasoning_type": "multi-hop"}}
